{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps    # import the spark suite\n",
    "import warnings         # display warning if spark context already exists\n",
    "import os\n",
    "\n",
    "import string\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- excerpt: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "row count:  9050\n",
      "+--------------+--------------------+---------------+\n",
      "|        author|             excerpt|          title|\n",
      "+--------------+--------------------+---------------+\n",
      "|CharlesDickens|A CHRISTMAS CAROL...|AChristmasCarol|\n",
      "|CharlesDickens|Mind! I don't mea...|AChristmasCarol|\n",
      "|CharlesDickens|Scrooge never pai...|AChristmasCarol|\n",
      "+--------------+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = 'data/data.json'\n",
    "raw_df = spark.read.json(data_file)\n",
    "\n",
    "raw_df.printSchema()\n",
    "print \"row count: \", raw_df.count()\n",
    "raw_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create copy of raw_df incase I mess things up :P\n",
    "df = raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Process the Excerpts and Create New Columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add word count, sentence count, avg word len, avg sent len,  . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define functions to apply to a row\n",
    "\n",
    "def char_count(text):\n",
    "    return len(text)\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def avg_word_length(text):\n",
    "    return sum([len(t) for t in text.split()]) / float(len(text.split()))\n",
    "\n",
    "def sentence_count(text):\n",
    "    return len(text.split('.'))\n",
    "\n",
    "# ? Could use sentence count col and word count col to do thie w/o udf\n",
    "def sentence_length(text):\n",
    "    return sum([len(t.split()) for t in text.split('.')]) / float(len(text.split('.')))\n",
    "\n",
    "# ? Create count of paragraphs per excerpt?\n",
    "def paragraph_count(text):\n",
    "    pass\n",
    "\n",
    "# create User Defined Functions from above\n",
    "charcount_udf = udf(lambda x : char_count(x))\n",
    "wordcount_udf = udf(lambda x: word_count(x))\n",
    "avgwordlen_udf = udf(lambda x: avg_word_length(x))\n",
    "sentencecount_udf = udf(lambda x: sentence_count(x))\n",
    "sentencelength_udf = udf(lambda x: sentence_length(x))\n",
    "\n",
    "# add columns to datafram\n",
    "\n",
    "df = df.withColumn(\"char_count\", charcount_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"word_count\", wordcount_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"avg_wordlen\", avgwordlen_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"sent_count\", sentencecount_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"sent_length\", sentencelength_udf(df.excerpt).cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+----------+----------+----------+-----------+\n",
      "|        author|             excerpt|char_count|word_count|sent_count|sent_length|\n",
      "+--------------+--------------------+----------+----------+----------+-----------+\n",
      "|CharlesDickens|A CHRISTMAS CAROL...|    1156.0|     214.0|      13.0|   16.76923|\n",
      "|CharlesDickens|Mind! I don't mea...|    1504.0|     268.0|      14.0|  19.142857|\n",
      "|CharlesDickens|Scrooge never pai...|    1438.0|     250.0|      16.0|     15.625|\n",
      "|CharlesDickens|Nobody ever stopp...|    1643.0|     303.0|       9.0|  33.666668|\n",
      "|CharlesDickens|The door of Scroo...|    1141.0|     211.0|      10.0|       21.1|\n",
      "+--------------+--------------------+----------+----------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "spark.sql('''\n",
    "    SELECT author, excerpt, char_count, word_count, sent_count, sent_length\n",
    "    FROM data''').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For experimentation purposes - here is how to get one excerpt from the spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "df.createOrReplaceTempView(\"data\")\n",
    "temp = spark.sql('''SELECT excerpt FROM data LIMIT 5''')\n",
    "sample_text = str(temp.take(3)[1].excerpt)\n",
    "temp.show()\n",
    "sample_text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark's Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for feature in result.select(\"result\").take(3):\n",
    "    print(feature)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"label\", \"words\"])\n",
    "ngram = NGram(inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "for ngrams_label in ngramDataFrame.select(\"ngrams\", \"label\").take(3):\n",
    "    print(ngrams_label)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Sparks MLlib NLP Functions to the Excerpts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation from excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CONSIDER: reworking with REGEX\n",
    "# CONSIDER: Remove Punctuation with stopwords (Tip From Sally)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # return text.translate(None, punctuation)\n",
    "    return \"\".join(c for c in text if c not in set(string.punctuation))\n",
    "\n",
    "removepunctuation_udf = udf(lambda x : remove_punctuation(x))\n",
    "\n",
    "df = df.withColumn(\"words_only\", removepunctuation_udf(df.excerpt).cast(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the punctuationless excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(inputCol=\"words_only\", outputCol=\"tokenized\")\n",
    "df = tokenizer.transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorize token lists in df (turn wordlist into a vector of word counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cv = CountVectorizer(inputCol=\"tokenized\", outputCol=\"count_vectorized\")\n",
    "cvmodel = cv.fit(df)\n",
    "df = cvmodel.transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "idf = IDF(inputCol=\"count_vectorized\", outputCol=\"tfidf\")\n",
    "idfmodel = idf.fit(df)\n",
    "df = idfmodel.transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The FINAL DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+\n",
      "|        author|           tokenized|    count_vectorized|               tfidf|\n",
      "+--------------+--------------------+--------------------+--------------------+\n",
      "|CharlesDickens|[a, christmas, ca...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[mind, i, dont, m...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[scrooge, never, ...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[nobody, ever, st...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[the, door, of, s...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[i, do, said, scr...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "+--------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "sqldf = spark.sql('''\n",
    "    SELECT author, tokenized, count_vectorized, tfidf\n",
    "    FROM data\n",
    "    LIMIT 6\n",
    "    ''')\n",
    "sqldf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqldf.write.save(\"save_test.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+\n",
      "|        author|           tokenized|    count_vectorized|               tfidf|\n",
      "+--------------+--------------------+--------------------+--------------------+\n",
      "|CharlesDickens|[a, christmas, ca...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[mind, i, dont, m...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[scrooge, never, ...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[nobody, ever, st...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[the, door, of, s...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[i, do, said, scr...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "+--------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM parquet.`save_test.parquet`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqldf.createOrReplaceTempView(\"sqldf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               tfidf|\n",
      "+--------------------+\n",
      "|(67321,[0,1,2,3,4...|\n",
      "|(67321,[0,1,2,3,4...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VERY SLOW\n",
    "spark.sql(\"SELECT tokenized FROM sqldf LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               tfidf|\n",
      "+--------------------+\n",
      "|(67321,[0,1,2,3,4...|\n",
      "|(67321,[0,1,2,3,4...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VERY FAST\n",
    "spark.sql(\"SELECT tokenized FROM parquet.`save_test.parquet` LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+\n",
      "|        author|           tokenized|    count_vectorized|               tfidf|\n",
      "+--------------+--------------------+--------------------+--------------------+\n",
      "|CharlesDickens|[a, christmas, ca...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[mind, i, dont, m...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[scrooge, never, ...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[nobody, ever, st...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[the, door, of, s...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "|CharlesDickens|[i, do, said, scr...|(67321,[0,1,2,3,4...|(67321,[0,1,2,3,4...|\n",
      "+--------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table = \"parquet.`save_test.parquet`\"\n",
    "spark.sql(\"SELECT * FROM {}\"\n",
    "          .format(table)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
