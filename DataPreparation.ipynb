{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from spacy.en import English\n",
    "from spacy.symbols import ORTH, LEMMA, POS, SYM, TAG\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType, IntegerType\n",
    "\n",
    "from src.spacy_transformer import SpacyTokenize_Transformer\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.pipeline import Transformer\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "# from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sql session setup by script:\t <pyspark.sql.session.SparkSession object at 0x109fd7fd0>\n",
      "spark context setup by script:\t <pyspark.context.SparkContext object at 0x1020dc278>\n",
      "pyspark imported by script:\t <module 'pyspark' from '/usr/local/Cellar/apache-spark/2 ...\n"
     ]
    }
   ],
   "source": [
    "# when starting jupyter with the sparkjupyter script, pyspark is already imported\n",
    "\n",
    "print(\"sql session setup by script:\\t\", spark)\n",
    "print(\"spark context setup by script:\\t\", sc)\n",
    "print(\"pyspark imported by script:\\t\", str(pyspark)[:56], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- excerpt: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "row count:  9050\n",
      "+--------------+--------------------+---------------+\n",
      "|        author|             excerpt|          title|\n",
      "+--------------+--------------------+---------------+\n",
      "|CharlesDickens|A CHRISTMAS CAROL...|AChristmasCarol|\n",
      "|CharlesDickens|Mind! I don't mea...|AChristmasCarol|\n",
      "|CharlesDickens|Scrooge never pai...|AChristmasCarol|\n",
      "+--------------+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = 'data/data.json'\n",
    "raw_df = spark.read.json(data_file)\n",
    "\n",
    "raw_df.printSchema()\n",
    "print(\"row count: \", raw_df.count())\n",
    "raw_df.show(3)\n",
    "\n",
    "\n",
    "# create copy of raw_df incase I mess things up :P\n",
    "df = raw_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "5\n",
      "+--------------+--------------------+--------------------+\n",
      "|        author|             excerpt|               title|\n",
      "+--------------+--------------------+--------------------+\n",
      "|     MarkTwain|The guards were l...|AConnecticutYanke...|\n",
      "|CharlesDickens|I looked at the o...|    DavidCopperfield|\n",
      "|CharlesDickens|‘Now, let me see,...|    DavidCopperfield|\n",
      "|CharlesDickens|This avenging pha...|   GreatExpectations|\n",
      "|CharlesDickens|So now, as an inf...|   GreatExpectations|\n",
      "+--------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a tiny sample dataframe for testing\n",
    "tiny_df = df.sample(False, 1/1000).limit(5)\n",
    "print(type(tiny_df))\n",
    "print(tiny_df.count())\n",
    "tiny_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spacy: a brief aside\n",
    "\n",
    "Spacy is a production oriented Natural Language Processing package with (among other things) very nice tokenization options. I use spaCy here because it tokenizes punctuation and contractions better than spark's tokenizer.\n",
    "\n",
    "Here we will wrap the tokenization in a Spark UDF. Later we will include it in our customized transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 76.6 ms, sys: 3.36 ms, total: 79.9 ms\n",
      "Wall time: 86.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# timing to ensure spaCy is set up properly (should take ~100ms)\n",
    "\n",
    "parser = English()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Grab a couple excerpts for testing\n",
    "\n",
    "excerpt = df.take(100)[80]['excerpt']\n",
    "excerpt2 = df.take(100)[99]['excerpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'they', 'did', \"n't\", 'devote', 'the', 'whole', 'evening']\n",
      "CPU times: user 11.9 ms, sys: 2.56 ms, total: 14.5 ms\n",
      "Wall time: 24.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsedData = parser(excerpt)\n",
    "\n",
    "# sentences = [sent.string.strip() for sent in parsedData.sents]\n",
    "# for s in sentences:\n",
    "#     print(s, '\\n')\n",
    "\n",
    "tokens = [tok.lower_ for tok in parsedData]\n",
    "# print(type(token_lower[1]))\n",
    "print(tokens[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## UDF demonstration\n",
    "A quick way to create a User Defined Function (UDF) in spark:\n",
    "\n",
    "Get (or create a function) in python and use a lambda function to insert it in to \"udf(  )\".\n",
    "\n",
    "Don't forget to define your Spark DataType!\n",
    "\n",
    "```\n",
    "Other excerpt metadata to include via UDF:\n",
    "num_chars, num_words, num_sent, num_para\n",
    "(use these to calc word_len, word_per_sent, word_per_para, sent_per_para . . . etc.\n",
    "per excerpt, book and author)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+\n",
      "|        author|             excerpt|               title|              tokens|\n",
      "+--------------+--------------------+--------------------+--------------------+\n",
      "|     MarkTwain|The guards were l...|AConnecticutYanke...|[the, guards, wer...|\n",
      "|CharlesDickens|I looked at the o...|    DavidCopperfield|[i, looked, at, t...|\n",
      "|CharlesDickens|‘Now, let me see,...|    DavidCopperfield|[‘, now, ,, let, ...|\n",
      "+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "CPU times: user 24.4 ms, sys: 5.94 ms, total: 30.3 ms\n",
      "Wall time: 5.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def tokenize(text):\n",
    "    parser = English()\n",
    "    return [tok.lower_ for tok in parser(text)]\n",
    "\n",
    "tokenize_udf = udf(lambda x: tokenize(x), ArrayType(StringType()))\n",
    "\n",
    "df_tokens = tiny_df.withColumn(\"tokens\", tokenize_udf(df.excerpt))\n",
    "df_tokens.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Transformers\n",
    "add explanation and example of transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Native Transformers\n",
    "\n",
    "Many of the transformers in Spark's ML lib are great. (Sadly tokenizer leaves punctuation attached to the preceding word.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+\n",
      "|        author|             excerpt|               title|           tokenized|\n",
      "+--------------+--------------------+--------------------+--------------------+\n",
      "|     MarkTwain|The guards were l...|AConnecticutYanke...|[the, guards, wer...|\n",
      "|CharlesDickens|I looked at the o...|    DavidCopperfield|[i, looked, at, t...|\n",
      "|CharlesDickens|‘Now, let me see,...|    DavidCopperfield|[‘now,, let, me, ...|\n",
      "|CharlesDickens|This avenging pha...|   GreatExpectations|[this, avenging, ...|\n",
      "|CharlesDickens|So now, as an inf...|   GreatExpectations|[so, now,, as, an...|\n",
      "+--------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"excerpt\", outputCol=\"tokenized\")\n",
    "df_spark_tokens = tokenizer.transform(tiny_df)\n",
    "df_spark_tokens.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Customized Transformers\n",
    "Luckily, we can make our own transformers as well.\n",
    "\n",
    "Here we build the spaCy tokenizer into a spark transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Spacy Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 898 µs, sys: 620 µs, total: 1.52 ms\n",
      "Wall time: 2.37 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = SpacyTokenize_Transformer(inputCol='excerpt', outputCol='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+---------------+--------------------+\n",
      "|        author|             excerpt|          title|               words|\n",
      "+--------------+--------------------+---------------+--------------------+\n",
      "|CharlesDickens|A CHRISTMAS CAROL...|AChristmasCarol|[A, CHRISTMAS, CA...|\n",
      "|CharlesDickens|Mind! I don't mea...|AChristmasCarol|[Mind, !, I, do, ...|\n",
      "|CharlesDickens|Scrooge never pai...|AChristmasCarol|[Scrooge, never, ...|\n",
      "+--------------+--------------------+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "CPU times: user 26.2 ms, sys: 6.18 ms, total: 32.4 ms\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_tokens = tokenizer.transform(df)\n",
    "df_tokens.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Pipeline\n",
    "\n",
    "Pipelines allow for multiple transformers to be strung together efficiently.\n",
    "\n",
    "By using \".getOutputCol( )\" column names can be set in a single location.\n",
    "\n",
    "Columns can then be added/dropped simply by adding or removing them from the \"stages\" list  in the Pipeline\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol=\"parsed_text\", outputCol=\"raw_tokens\"\n",
    "            , pattern=\"\\\\W\", minTokenLength=3)\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='tokens_stop')\n",
    "stemmer = Stemming_Transformer(inputCol=remover.getOutputCol(), outputCol='tokens')\n",
    "bigram = NGram(inputCol=stemmer.getOutputCol(), outputCol='bigrams'\n",
    "         , n=2)\n",
    "trigram = NGram(inputCol=stemmer.getOutputCol(), outputCol='trigrams'\n",
    "          , n=3)\n",
    "cv = CountVectorizer(inputCol=stemmer.getOutputCol(), outputCol='token_countvector'\n",
    "     , minDF=10.0)\n",
    "idf = IDF(inputCol=cv.getOutputCol(), outputCol='token_idf'\n",
    "      , minDocFreq=10)\n",
    "w2v_2d = Word2Vec(vectorSize=2, minCount=2, inputCol=stemmer.getOutputCol()\n",
    "         , outputCol='word2vec_2d')\n",
    "w2v_large = Word2Vec(vectorSize=250, minCount=2, inputCol=stemmer.getOutputCol()\n",
    "            , outputCol='word2vec_large')\n",
    "\n",
    "pipe = Pipeline(stages=[tokenizer, remover, stemmer, cv, idf, w2v_2d, w2v_large])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set up transformers\n",
    "tokenizer = SpacyTokenize_Transformer(inputCol='excerpt', outputCol='words')\n",
    "countvec = CountVectorizer(inputCol=tokenizer.getOutputCol(), outputCol='termfreq')\n",
    "idf = IDF(inputCol=countvec.getOutputCol(), outputCol='tfidf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        author|             excerpt|               title|               words|            termfreq|               tfidf|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     MarkTwain|The guards were l...|AConnecticutYanke...|[The, guards, wer...|(609,[0,1,2,3,4,5...|(609,[0,1,2,3,4,5...|\n",
      "|CharlesDickens|I looked at the o...|    DavidCopperfield|[I, looked, at, t...|(609,[0,1,2,3,4,5...|(609,[0,1,2,3,4,5...|\n",
      "|CharlesDickens|‘Now, let me see,...|    DavidCopperfield|[‘, Now, ,, let, ...|(609,[0,1,2,3,4,5...|(609,[0,1,2,3,4,5...|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "CPU times: user 48.5 ms, sys: 9.45 ms, total: 58 ms\n",
      "Wall time: 7.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build pipeline and run pipeline\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, countvec, idf])\n",
    "# data = pipeline.fit(tiny_df).transform(tiny_df)\n",
    "data = pipeline.fit(tiny_df).transform(tiny_df)\n",
    "\n",
    "data.show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Note:\n",
    "%%time output for full df:\n",
    "```\n",
    "CPU times: user 136 ms, sys: 80.6 ms, total: 217 ms\n",
    "Wall time: 22min 14s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
