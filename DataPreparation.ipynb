{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-2-a205a89a3599>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-a205a89a3599>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    print \"sql session setup by script:\\t\", spark\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "# when starting jupyter with the sparkjupyter script, pyspark is already imported\n",
    "\n",
    "print \"sql session setup by script:\\t\", spark\n",
    "print \"spark context setup by script:\\t\", sc\n",
    "print \"pyspark imported by script:\\t\", str(pyspark)[:56], \"...\"\n",
    "\n",
    "# ps= pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- excerpt: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "row count:  9050\n",
      "+--------------+--------------------+---------------+\n",
      "|        author|             excerpt|          title|\n",
      "+--------------+--------------------+---------------+\n",
      "|CharlesDickens|A CHRISTMAS CAROL...|AChristmasCarol|\n",
      "|CharlesDickens|Mind! I don't mea...|AChristmasCarol|\n",
      "|CharlesDickens|Scrooge never pai...|AChristmasCarol|\n",
      "+--------------+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = 'data/data.json'\n",
    "raw_df = spark.read.json(data_file)\n",
    "\n",
    "raw_df.printSchema()\n",
    "print(\"row count: \", raw_df.count())\n",
    "raw_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create copy of raw_df incase I mess things up :P\n",
    "df = raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Process the Excerpts and Create New Columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Add word count, sentence count, avg word len, avg sent len,  . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define functions to apply to a row\n",
    "\n",
    "def char_count(text):\n",
    "    return len(text)\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def avg_word_length(text):\n",
    "    return sum([len(t) for t in text.split()]) / float(len(text.split()))\n",
    "\n",
    "def sentence_count(text):\n",
    "    return len(re.split('[?.!]', text))\n",
    "\n",
    "# ? Could use sentence count col and word count col to do thie w/o udf\n",
    "def sentence_length(text):\n",
    "    sentences = re.split('[?.!]', text)\n",
    "    return sum([len(t.split()) for t in sentences]) / float(len(sentences))\n",
    "\n",
    "# ? Create count of paragraphs per excerpt?\n",
    "def paragraph_count(text):\n",
    "    pass\n",
    "\n",
    "# create User Defined Functions from above\n",
    "charcount_udf = udf(lambda x : char_count(x))\n",
    "wordcount_udf = udf(lambda x: word_count(x))\n",
    "avgwordlen_udf = udf(lambda x: avg_word_length(x))\n",
    "sentencecount_udf = udf(lambda x: sentence_count(x))\n",
    "sentencelength_udf = udf(lambda x: sentence_length(x))\n",
    "\n",
    "# add columns to dataframe\n",
    "df = df.withColumn(\"char_count\", charcount_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"word_count\", wordcount_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"avg_wordlen\", avgwordlen_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"sent_count\", sentencecount_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"avg_sentlen\", sentencelength_udf(df.excerpt).cast(FloatType())).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#FIXME: we are not actually calculating the averages properly here \n",
    "#  - this is an average of averages!\n",
    "\n",
    "df.createOrReplaceTempView(\"data\")\n",
    "spark.sql('''\n",
    "            SELECT author\n",
    "                    , ROUND(AVG(avg_wordlen),3) AS AvgWordLen\n",
    "                    , ROUND(AVG(word_count),1) AS AvgWordsPerPara\n",
    "                    , ROUND(AVG(sent_length),1) AS AvgWordsPerSent\n",
    "            FROM data\n",
    "            GROUP BY author\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### For experimentation purposes - here is how to get one excerpt from the spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "```python\n",
    "df.createOrReplaceTempView(\"data\")\n",
    "temp = spark.sql('''SELECT excerpt FROM data LIMIT 5''')\n",
    "sample_text = str(temp.take(3)[1].excerpt)\n",
    "temp.show()\n",
    "sample_text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spark's Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tokenizer  <br> regexTokenizer\n",
    "```python\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### word2vec\n",
    "\n",
    "```python\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for feature in result.select(\"result\").take(3):\n",
    "    print(feature)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### n-grams\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"label\", \"words\"])\n",
    "ngram = NGram(inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "for ngrams_label in ngramDataFrame.select(\"ngrams\", \"label\").take(3):\n",
    "    print(ngrams_label)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Applying Sparks MLlib NLP Functions to the Excerpts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Remove Punctuation from excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# CONSIDER: reworking with REGEX\n",
    "# CONSIDER: Remove Punctuation with stopwords (Tip From Sally)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # return text.translate(None, punctuation)\n",
    "\n",
    "    # this avoids error in spark due to apparent 'translate' name collision\n",
    "    # Would it be more efficient to use .stip() on each word after tokenize?\n",
    "    return \"\".join(c for c in text if c not in set(string.punctuation))\n",
    "\n",
    "\n",
    "removepunctuation_udf = udf(lambda x : remove_punctuation(x))\n",
    "\n",
    "df = df.withColumn(\"words_only\", removepunctuation_udf(df.excerpt).cast(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tokenize the punctuationless excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(inputCol=\"words_only\", outputCol=\"tokenized\")\n",
    "df = tokenizer.transform(df).persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### regexTokenizer\n",
    "#### May be worth playing with later - to split and remove punctuation in one step <br> and to handle cases like church-yard vs. instance--literally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df= df.drop('tokenized')\n",
    "\n",
    "# re_Tokenizer = RegexTokenizer(inputCol=\"excerpt\", outputCol=\"tokenized\", pattern=\"\\\\w+\")\n",
    "# # re_tokenizer = RegexTokenizer(inputCol=\"excerpt\", outputCol=\"tokenized\")\n",
    "# df = re_tokenizer.transform(df).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+\n",
      "|        author|             excerpt|           tokenized|\n",
      "+--------------+--------------------+--------------------+\n",
      "|CharlesDickens|A CHRISTMAS CAROL...|[a, christmas, ca...|\n",
      "|CharlesDickens|Mind! I don't mea...|[mind, i, dont, m...|\n",
      "|CharlesDickens|Scrooge never pai...|[scrooge, never, ...|\n",
      "|CharlesDickens|Nobody ever stopp...|[nobody, ever, st...|\n",
      "|CharlesDickens|The door of Scroo...|[the, door, of, s...|\n",
      "|CharlesDickens|\"I do,\" said Scro...|[i, do, said, scr...|\n",
      "|CharlesDickens|\"Uncle!\" pleaded ...|[uncle, pleaded, ...|\n",
      "|CharlesDickens|The clerk in the ...|[the, clerk, in, ...|\n",
      "|CharlesDickens|\"I am sorry, with...|[i, am, sorry, wi...|\n",
      "|CharlesDickens|\"Mr. Marley has b...|[mr, marley, has,...|\n",
      "+--------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "temp = spark.sql('''\n",
    "    SELECT author, excerpt, tokenized\n",
    "    FROM data \n",
    "    LIMIT 10\n",
    "    ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Remove stop words?\n",
    "##### Stop words left in for the moment on the theory/suspicion that authors' use these terms may be relevant to identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remover = StopWordsRemover(inputCol=\"tokenized\", outputCol=\"words_nostops\")\n",
    "# df = remover.transform(df).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### CountVectorize token lists in df (turn wordlist into a vector of word counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cv = CountVectorizer(inputCol=\"tokenized\", outputCol=\"count_vectorized\")\n",
    "cvmodel = cv.fit(df)\n",
    "df = cvmodel.transform(df).persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Calculate Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "idf = IDF(inputCol=\"count_vectorized\", outputCol=\"tfidf\")\n",
    "idfmodel = idf.fit(df)\n",
    "df = idfmodel.transform(df).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(author='CharlesDickens', excerpt='\"Mr. Marley has been dead these seven years,\" Scrooge replied. \"He died seven years ago, this very night.\" || \"We have no doubt his liberality is well represented by his surviving partner,\" said the gentleman, presenting his credentials. || It certainly was; for they had been two kindred spirits. At the ominous word \"liberality\" Scrooge frowned, and shook his head, and handed the credentials back. || \"At this festive season of the year, Mr. Scrooge,\" said the gentleman, taking up a pen, \"it is more than usually desirable that we should make some slight provision for the poor and destitute, who suffer greatly at the present time. Many thousands are in want of common necessaries; hundreds of thousands are in want of common comforts, sir.\" || \"Are there no prisons?\" asked Scrooge. || \"Plenty of prisons,\" said the gentleman, laying down the pen again. || \"And the Union workhouses?\" demanded Scrooge. \"Are they still in operation?\" || \"They are. Still,\" returned the gentleman, \"I wish I could say they were not.\" || \"The Treadmill and the Poor Law are in full vigour, then?\" said Scrooge. || \"Both very busy, sir.\" || \"Oh! I was afraid, from what you said at first, that something had occurred to stop them in their useful course,\" said Scrooge. \"I am very glad to hear it.\"', title='AChristmasCarol', words_only='Mr Marley has been dead these seven years Scrooge replied He died seven years ago this very night  We have no doubt his liberality is well represented by his surviving partner said the gentleman presenting his credentials  It certainly was for they had been two kindred spirits At the ominous word liberality Scrooge frowned and shook his head and handed the credentials back  At this festive season of the year Mr Scrooge said the gentleman taking up a pen it is more than usually desirable that we should make some slight provision for the poor and destitute who suffer greatly at the present time Many thousands are in want of common necessaries hundreds of thousands are in want of common comforts sir  Are there no prisons asked Scrooge  Plenty of prisons said the gentleman laying down the pen again  And the Union workhouses demanded Scrooge Are they still in operation  They are Still returned the gentleman I wish I could say they were not  The Treadmill and the Poor Law are in full vigour then said Scrooge  Both very busy sir  Oh I was afraid from what you said at first that something had occurred to stop them in their useful course said Scrooge I am very glad to hear it', tokenized=['mr', 'marley', 'has', 'been', 'dead', 'these', 'seven', 'years', 'scrooge', 'replied', 'he', 'died', 'seven', 'years', 'ago', 'this', 'very', 'night', '', 'we', 'have', 'no', 'doubt', 'his', 'liberality', 'is', 'well', 'represented', 'by', 'his', 'surviving', 'partner', 'said', 'the', 'gentleman', 'presenting', 'his', 'credentials', '', 'it', 'certainly', 'was', 'for', 'they', 'had', 'been', 'two', 'kindred', 'spirits', 'at', 'the', 'ominous', 'word', 'liberality', 'scrooge', 'frowned', 'and', 'shook', 'his', 'head', 'and', 'handed', 'the', 'credentials', 'back', '', 'at', 'this', 'festive', 'season', 'of', 'the', 'year', 'mr', 'scrooge', 'said', 'the', 'gentleman', 'taking', 'up', 'a', 'pen', 'it', 'is', 'more', 'than', 'usually', 'desirable', 'that', 'we', 'should', 'make', 'some', 'slight', 'provision', 'for', 'the', 'poor', 'and', 'destitute', 'who', 'suffer', 'greatly', 'at', 'the', 'present', 'time', 'many', 'thousands', 'are', 'in', 'want', 'of', 'common', 'necessaries', 'hundreds', 'of', 'thousands', 'are', 'in', 'want', 'of', 'common', 'comforts', 'sir', '', 'are', 'there', 'no', 'prisons', 'asked', 'scrooge', '', 'plenty', 'of', 'prisons', 'said', 'the', 'gentleman', 'laying', 'down', 'the', 'pen', 'again', '', 'and', 'the', 'union', 'workhouses', 'demanded', 'scrooge', 'are', 'they', 'still', 'in', 'operation', '', 'they', 'are', 'still', 'returned', 'the', 'gentleman', 'i', 'wish', 'i', 'could', 'say', 'they', 'were', 'not', '', 'the', 'treadmill', 'and', 'the', 'poor', 'law', 'are', 'in', 'full', 'vigour', 'then', 'said', 'scrooge', '', 'both', 'very', 'busy', 'sir', '', 'oh', 'i', 'was', 'afraid', 'from', 'what', 'you', 'said', 'at', 'first', 'that', 'something', 'had', 'occurred', 'to', 'stop', 'them', 'in', 'their', 'useful', 'course', 'said', 'scrooge', 'i', 'am', 'very', 'glad', 'to', 'hear', 'it'], count_vectorized=SparseVector(66978, {0: 13.0, 1: 5.0, 2: 5.0, 3: 2.0, 4: 1.0, 5: 4.0, 6: 5.0, 7: 2.0, 8: 10.0, 9: 3.0, 10: 2.0, 11: 1.0, 12: 4.0, 16: 2.0, 17: 1.0, 18: 2.0, 19: 1.0, 21: 4.0, 26: 2.0, 27: 1.0, 32: 1.0, 33: 6.0, 34: 4.0, 35: 1.0, 36: 2.0, 37: 2.0, 38: 1.0, 41: 2.0, 42: 2.0, 45: 1.0, 47: 1.0, 48: 2.0, 50: 1.0, 52: 3.0, 54: 1.0, 55: 1.0, 56: 1.0, 57: 6.0, 58: 1.0, 63: 1.0, 66: 1.0, 67: 1.0, 68: 1.0, 69: 1.0, 72: 1.0, 87: 1.0, 92: 1.0, 94: 1.0, 98: 1.0, 100: 1.0, 108: 1.0, 110: 1.0, 114: 1.0, 118: 1.0, 127: 1.0, 130: 1.0, 134: 1.0, 141: 1.0, 148: 2.0, 160: 1.0, 168: 2.0, 193: 1.0, 208: 2.0, 209: 1.0, 217: 2.0, 222: 1.0, 242: 1.0, 244: 1.0, 246: 1.0, 249: 2.0, 259: 1.0, 278: 1.0, 283: 1.0, 287: 4.0, 310: 1.0, 320: 1.0, 360: 1.0, 366: 1.0, 378: 1.0, 382: 1.0, 401: 1.0, 409: 1.0, 443: 1.0, 469: 1.0, 482: 1.0, 495: 2.0, 500: 1.0, 693: 2.0, 698: 7.0, 735: 1.0, 796: 1.0, 850: 1.0, 1013: 1.0, 1137: 1.0, 1178: 1.0, 1246: 1.0, 1266: 1.0, 1304: 1.0, 1363: 1.0, 1390: 1.0, 1512: 1.0, 1560: 1.0, 1669: 2.0, 1882: 1.0, 1962: 2.0, 2025: 1.0, 2186: 1.0, 2343: 1.0, 2361: 1.0, 2450: 1.0, 3503: 1.0, 4083: 1.0, 4207: 1.0, 5386: 1.0, 5465: 1.0, 6118: 1.0, 6140: 2.0, 7223: 1.0, 7278: 2.0, 8265: 1.0, 8722: 1.0, 10643: 1.0, 11076: 1.0, 11881: 1.0, 12172: 1.0, 13805: 1.0, 14634: 1.0, 16911: 2.0, 25884: 1.0, 28386: 1.0}), tfidf=SparseVector(66978, {0: 0.046, 1: 0.0155, 2: 0.0567, 3: 0.0111, 4: 0.0136, 5: 1.0611, 6: 0.1338, 7: 0.2766, 8: 0.8855, 9: 0.2754, 10: 0.1844, 11: 0.3703, 12: 1.6007, 16: 0.6063, 17: 0.6386, 18: 0.4082, 19: 0.3278, 21: 1.0819, 26: 1.1705, 27: 0.4467, 32: 0.4817, 33: 3.7224, 34: 2.9179, 35: 0.6163, 36: 1.8713, 37: 1.0647, 38: 0.5825, 41: 1.2815, 42: 2.0608, 45: 0.6864, 47: 0.7506, 48: 1.5332, 50: 0.8545, 52: 2.4625, 54: 0.8337, 55: 0.8279, 56: 1.0689, 57: 6.495, 58: 0.9194, 63: 1.0548, 66: 1.0329, 67: 1.0871, 68: 1.0631, 69: 1.0571, 72: 1.1693, 87: 1.2966, 92: 1.3713, 94: 1.4899, 98: 1.3241, 100: 1.3224, 108: 1.4272, 110: 1.5888, 114: 1.4444, 118: 1.5223, 127: 1.5871, 130: 1.5631, 134: 1.7009, 141: 1.7051, 148: 3.4998, 160: 1.9215, 168: 4.6705, 193: 2.0164, 208: 4.4957, 209: 2.251, 217: 4.579, 222: 2.2302, 242: 2.3479, 244: 2.3844, 246: 2.3138, 249: 4.9001, 259: 2.4038, 278: 2.4708, 283: 2.5108, 287: 11.0042, 310: 2.5217, 320: 2.5581, 360: 2.7493, 366: 2.8472, 378: 2.7442, 382: 2.9081, 401: 2.9327, 409: 2.8228, 443: 2.9306, 469: 3.0376, 482: 3.0422, 495: 6.1695, 500: 3.0751, 693: 6.9446, 698: 31.7514, 735: 3.4652, 796: 3.5892, 850: 3.651, 1013: 3.7779, 1137: 4.1067, 1178: 3.9011, 1246: 3.9807, 1266: 4.0355, 1304: 4.0355, 1363: 4.0934, 1390: 4.1478, 1512: 4.198, 1560: 4.2664, 1669: 8.9133, 1882: 4.5055, 1962: 9.2666, 2025: 4.5359, 2186: 4.622, 2343: 4.6563, 2361: 4.7799, 2450: 4.7539, 3503: 5.1033, 4083: 5.304, 4207: 5.3494, 5386: 5.6766, 5465: 5.7094, 6118: 5.8148, 6140: 11.7051, 7223: 6.4026, 7278: 12.3324, 8265: 6.2203, 8722: 6.338, 10643: 6.6257, 11076: 6.6257, 11881: 6.808, 12172: 6.808, 13805: 7.0312, 14634: 7.0312, 16911: 15.0024, 25884: 8.012, 28386: 8.012}))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(10)[9][]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"tokenized\", outputCol=\"w2v\")\n",
    "w2vmodel = word2Vec.fit(df)\n",
    "df = w2vmodel.transform(df).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ngram = NGram(n=2, inputCol=\"tokenized\", outputCol=\"2_gram\")\n",
    "df = ngram.transform(df)\n",
    "\n",
    "ngram = NGram(n=3, inputCol=\"tokenized\", outputCol=\"3_gram\")\n",
    "df = ngram.transform(df)\n",
    "\n",
    "ngram = NGram(n=4, inputCol=\"tokenized\", outputCol=\"4_gram\")\n",
    "df = ngram.transform(df)\n",
    "\n",
    "ngram = NGram(n=5, inputCol=\"tokenized\", outputCol=\"5_gram\")\n",
    "df = ngram.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The FINAL DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "sqldf = spark.sql('''\n",
    "    SELECT tokenized, count_vectorized, tfidf, w2v\n",
    "    FROM data\n",
    "    LIMIT 6\n",
    "    ''')\n",
    "sqldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|           tokenized|              2_gram|              5_gram|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[a, christmas, ca...|[a christmas, chr...|[a christmas caro...|\n",
      "|[mind, i, dont, m...|[mind i, i dont, ...|[mind i dont mean...|\n",
      "|[scrooge, never, ...|[scrooge never, n...|[scrooge never pa...|\n",
      "|[nobody, ever, st...|[nobody ever, eve...|[nobody ever stop...|\n",
      "|[the, door, of, s...|[the door, door o...|[the door of scro...|\n",
      "|[i, do, said, scr...|[i do, do said, s...|[i do said scroog...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "sqldf = spark.sql('''\n",
    "    SELECT tokenized, 2_gram, 5_gram\n",
    "    FROM data\n",
    "    LIMIT 6\n",
    "    ''')\n",
    "sqldf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sqldf.write.save(\"data/save_test.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM parquet.`data/save_test.parquet`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sqldf.createOrReplaceTempView(\"sqldf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# not too slow anymore\n",
    "spark.sql(\"SELECT tokenized FROM data LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# VERY FAST (direct parquet access)\n",
    "spark.sql(\"SELECT tokenized FROM parquet.`data/save_test.parquet`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Use formatting to set table name\n",
    "\n",
    "table = \"parquet.`data/save_test.parquet`\"\n",
    "\n",
    "spark.sql(\"SELECT * FROM {}\"\n",
    "          .format(table)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Query Data directly from Parquet - store data as pandas for plotting etc.\n",
    "\n",
    "(MOVE to Explore and Visualize notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pddf = spark.sql(\"SELECT * FROM {}\"\n",
    "          .format(table)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pddf.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Saving the Final DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').save(\"data/excerpt_df.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
