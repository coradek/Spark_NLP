{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType, IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.pipeline import Transformer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "from spacy.en import English\n",
    "\n",
    "from src.spacy_transformer import SpacyTokenize_Transformer\n",
    "from src.NLP_pipeline import get_pipeline\n",
    "\n",
    "# from pyspark.ml.feature import Word2Vec\n",
    "# from pyspark.ml.feature import NGram\n",
    "# from spacy.symbols import ORTH, LEMMA, POS, SYM, TAG\n",
    "# import pandas as pd\n",
    "# from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sql session setup by script:\t <pyspark.sql.session.SparkSession object at 0x10a112fd0>\n",
      "spark context setup by script:\t <pyspark.context.SparkContext object at 0x1022dc278>\n",
      "pyspark imported by script:\t <module 'pyspark' from '/usr/local/Cellar/apache-spark/2 ...\n"
     ]
    }
   ],
   "source": [
    "# when starting jupyter with the sparkjupyter script, pyspark is already imported\n",
    "\n",
    "print(\"sql session setup by script:\\t\", spark)\n",
    "print(\"spark context setup by script:\\t\", sc)\n",
    "print(\"pyspark imported by script:\\t\", str(pyspark)[:56], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- excerpt: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "row count:  9050\n",
      "+--------------+--------------------+---------------+\n",
      "|        author|             excerpt|          title|\n",
      "+--------------+--------------------+---------------+\n",
      "|CharlesDickens|A CHRISTMAS CAROL...|AChristmasCarol|\n",
      "|CharlesDickens|Mind! I don't mea...|AChristmasCarol|\n",
      "|CharlesDickens|Scrooge never pai...|AChristmasCarol|\n",
      "+--------------+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = 'data/data.json'\n",
    "raw_df = spark.read.json(data_file)\n",
    "\n",
    "raw_df.printSchema()\n",
    "print(\"row count: \", raw_df.count())\n",
    "raw_df.show(3)\n",
    "\n",
    "\n",
    "# create copy of raw_df incase I mess things up :P\n",
    "df = raw_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "5\n",
      "+--------------+--------------------+----------------+\n",
      "|        author|             excerpt|           title|\n",
      "+--------------+--------------------+----------------+\n",
      "|CharlesDickens|He shook his head...|ATaleOfTwoCities|\n",
      "|CharlesDickens|‘I am very poor,’...|DavidCopperfield|\n",
      "|CharlesDickens|‘It’s work enough...|DavidCopperfield|\n",
      "|    JaneAusten|There was no occa...|            Emma|\n",
      "|    JaneAusten|“Oh! shame, shame...|   MansfieldPark|\n",
      "+--------------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a tiny sample dataframe for testing\n",
    "tiny_df = df.sample(False, 1/1000).limit(5)\n",
    "print(type(tiny_df))\n",
    "print(tiny_df.count())\n",
    "tiny_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spacy: a brief aside\n",
    "\n",
    "Spacy is a production oriented Natural Language Processing package with (among other things) very nice tokenization options. I use spaCy here because it tokenizes punctuation and contractions better than spark's tokenizer.\n",
    "\n",
    "Here we will wrap the tokenization in a Spark UDF. Later we will include it in our customized transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 105 ms, sys: 4.05 ms, total: 109 ms\n",
      "Wall time: 115 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# timing to ensure spaCy is set up properly (should take ~100ms)\n",
    "\n",
    "parser = English()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Grab a couple excerpts for testing\n",
    "\n",
    "excerpt = df.take(100)[80]['excerpt']\n",
    "excerpt2 = df.take(100)[99]['excerpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'they', 'did', \"n't\", 'devote', 'the', 'whole', 'evening']\n",
      "CPU times: user 13.6 ms, sys: 1.94 ms, total: 15.5 ms\n",
      "Wall time: 19.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsedData = parser(excerpt)\n",
    "\n",
    "# sentences = [sent.string.strip() for sent in parsedData.sents]\n",
    "# for s in sentences:\n",
    "#     print(s, '\\n')\n",
    "\n",
    "tokens = [tok.lower_ for tok in parsedData]\n",
    "# print(type(token_lower[1]))\n",
    "print(tokens[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## UDF demonstration\n",
    "A quick way to create a User Defined Function (UDF) in spark:\n",
    "\n",
    "Get (or create a function) in python and use a lambda function to insert it in to \"udf(  )\".\n",
    "\n",
    "Don't forget to define your Spark DataType!\n",
    "\n",
    "```\n",
    "Other excerpt metadata to include via UDF:\n",
    "num_chars, num_words, num_sent, num_para\n",
    "(use these to calc word_len, word_per_sent, word_per_para, sent_per_para . . . etc.\n",
    "per excerpt, book and author)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+----------------+--------------------+\n",
      "|        author|             excerpt|           title|              tokens|\n",
      "+--------------+--------------------+----------------+--------------------+\n",
      "|CharlesDickens|He shook his head...|ATaleOfTwoCities|[he, shook, his, ...|\n",
      "|CharlesDickens|‘I am very poor,’...|DavidCopperfield|[‘, i, am, very, ...|\n",
      "|CharlesDickens|‘It’s work enough...|DavidCopperfield|[‘, it, ’s, work,...|\n",
      "+--------------+--------------------+----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "CPU times: user 20.2 ms, sys: 4.77 ms, total: 25 ms\n",
      "Wall time: 4.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def tokenize(text):\n",
    "    parser = English()\n",
    "    return [tok.lower_ for tok in parser(text)]\n",
    "\n",
    "tokenize_udf = udf(lambda x: tokenize(x), ArrayType(StringType()))\n",
    "\n",
    "df_tokens = tiny_df.withColumn(\"tokens\", tokenize_udf(df.excerpt))\n",
    "df_tokens.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Transformers\n",
    "add explanation and example of transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Native Transformers\n",
    "\n",
    "Many of the transformers in Spark's ML lib are great. (Sadly tokenizer leaves punctuation attached to the preceding word.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+----------------+--------------------+\n",
      "|        author|             excerpt|           title|           tokenized|\n",
      "+--------------+--------------------+----------------+--------------------+\n",
      "|CharlesDickens|He shook his head...|ATaleOfTwoCities|[he, shook, his, ...|\n",
      "|CharlesDickens|‘I am very poor,’...|DavidCopperfield|[‘i, am, very, po...|\n",
      "|CharlesDickens|‘It’s work enough...|DavidCopperfield|[‘it’s, work, eno...|\n",
      "|    JaneAusten|There was no occa...|            Emma|[there, was, no, ...|\n",
      "|    JaneAusten|“Oh! shame, shame...|   MansfieldPark|[“oh!, shame,, sh...|\n",
      "+--------------+--------------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"excerpt\", outputCol=\"tokenized\")\n",
    "df_spark_tokens = tokenizer.transform(tiny_df)\n",
    "df_spark_tokens.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Customized Transformers\n",
    "Luckily, we can make our own transformers as well.\n",
    "\n",
    "Here we build the spaCy tokenizer into a spark transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Spacy Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 558 µs, sys: 161 µs, total: 719 µs\n",
      "Wall time: 613 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = SpacyTokenize_Transformer(inputCol='excerpt', outputCol='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+---------------+--------------------+\n",
      "|        author|             excerpt|          title|               words|\n",
      "+--------------+--------------------+---------------+--------------------+\n",
      "|CharlesDickens|A CHRISTMAS CAROL...|AChristmasCarol|[A, CHRISTMAS, CA...|\n",
      "|CharlesDickens|Mind! I don't mea...|AChristmasCarol|[Mind, !, I, do, ...|\n",
      "|CharlesDickens|Scrooge never pai...|AChristmasCarol|[Scrooge, never, ...|\n",
      "+--------------+--------------------+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "CPU times: user 23 ms, sys: 4.76 ms, total: 27.8 ms\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_tokens = tokenizer.transform(df)\n",
    "df_tokens.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Pipeline\n",
    "\n",
    "Pipelines allow for multiple transformers to be strung together efficiently.\n",
    "\n",
    "By using \".getOutputCol( )\" column names can be set in a single location.\n",
    "\n",
    "Columns can then be added/dropped simply by adding or removing them from the \"stages\" list  in the Pipeline\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol=\"parsed_text\", outputCol=\"raw_tokens\"\n",
    "            , pattern=\"\\\\W\", minTokenLength=3)\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='tokens_stop')\n",
    "stemmer = Stemming_Transformer(inputCol=remover.getOutputCol(), outputCol='tokens')\n",
    "bigram = NGram(inputCol=stemmer.getOutputCol(), outputCol='bigrams'\n",
    "         , n=2)\n",
    "trigram = NGram(inputCol=stemmer.getOutputCol(), outputCol='trigrams'\n",
    "          , n=3)\n",
    "cv = CountVectorizer(inputCol=stemmer.getOutputCol(), outputCol='token_countvector'\n",
    "     , minDF=10.0)\n",
    "idf = IDF(inputCol=cv.getOutputCol(), outputCol='token_idf'\n",
    "      , minDocFreq=10)\n",
    "w2v_2d = Word2Vec(vectorSize=2, minCount=2, inputCol=stemmer.getOutputCol()\n",
    "         , outputCol='word2vec_2d')\n",
    "w2v_large = Word2Vec(vectorSize=250, minCount=2, inputCol=stemmer.getOutputCol()\n",
    "            , outputCol='word2vec_large')\n",
    "\n",
    "pipe = Pipeline(stages=[tokenizer, remover, stemmer, cv, idf, w2v_2d, w2v_large])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set up transformers\n",
    "tokenizer = SpacyTokenize_Transformer(inputCol='excerpt', outputCol='words')\n",
    "countvec = CountVectorizer(inputCol=tokenizer.getOutputCol(), outputCol='termfreq')\n",
    "idf = IDF(inputCol=countvec.getOutputCol(), outputCol='tfidf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+----------------+--------------------+--------------------+--------------------+\n",
      "|        author|             excerpt|           title|               words|            termfreq|               tfidf|\n",
      "+--------------+--------------------+----------------+--------------------+--------------------+--------------------+\n",
      "|CharlesDickens|He shook his head...|ATaleOfTwoCities|[He, shook, his, ...|(517,[0,1,2,3,4,5...|(517,[0,1,2,3,4,5...|\n",
      "|CharlesDickens|‘I am very poor,’...|DavidCopperfield|[‘, I, am, very, ...|(517,[0,1,2,3,4,5...|(517,[0,1,2,3,4,5...|\n",
      "|CharlesDickens|‘It’s work enough...|DavidCopperfield|[‘, It, ’s, work,...|(517,[0,1,2,3,4,5...|(517,[0,1,2,3,4,5...|\n",
      "|    JaneAusten|There was no occa...|            Emma|[There, was, no, ...|(517,[0,1,2,4,5,6...|(517,[0,1,2,4,5,6...|\n",
      "|    JaneAusten|“Oh! shame, shame...|   MansfieldPark|[“, Oh, !, shame,...|(517,[0,1,2,3,4,5...|(517,[0,1,2,3,4,5...|\n",
      "+--------------+--------------------+----------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "CPU times: user 52.2 ms, sys: 10.2 ms, total: 62.3 ms\n",
      "Wall time: 5.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# How to have only tfidf col added to df?\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, countvec, idf])\n",
    "data = pipeline.fit(tiny_df).transform(tiny_df)\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Note:\n",
    "%%time output for full df:\n",
    "```\n",
    "CPU times: user 136 ms, sys: 80.6 ms, total: 217 ms\n",
    "Wall time: 22min 14s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Building the full dataframe:\n",
    "\n",
    "With our full pipeline defined in a script we can now save the resulting dataframe to a parquet file for easy access in future notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 578 ms, sys: 358 ms, total: 936 ms\n",
      "Wall time: 2h 12min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# started 2:07 4/12\n",
    "# save_loc = \"data/dataframe.parquet\"\n",
    "\n",
    "pipeline = get_pipeline()\n",
    "df = pipeline.fit(raw_df).transform(raw_df)\n",
    "df.write.mode('overwrite').save(save_loc, format=\"parquet\")\n",
    "\n",
    "# Consider: Add in spark sql querries for some of the more interesting columns (just for kicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 12))\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "\"expression 'data.`excerpt`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;\\nGlobalLimit 5\\n+- LocalLimit 5\\n   +- Aggregate [author#0], [author#0, excerpt#1, title#2, words#573, termfreq#579, tfidf#586, w2v#594, w2v_2D#603]\\n      +- SubqueryAlias data\\n         +- Project [author#0, excerpt#1, title#2, words#573, termfreq#579, tfidf#586, w2v#594, UDF(words#573) AS w2v_2D#603]\\n            +- Project [author#0, excerpt#1, title#2, words#573, termfreq#579, tfidf#586, UDF(words#573) AS w2v#594]\\n               +- Project [author#0, excerpt#1, title#2, words#573, termfreq#579, UDF(termfreq#579) AS tfidf#586]\\n                  +- Project [author#0, excerpt#1, title#2, words#573, UDF(words#573) AS termfreq#579]\\n                     +- Project [author#0, excerpt#1, title#2, f(excerpt#1) AS words#573]\\n                        +- Relation[author#0,excerpt#1,title#2] json\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o26.sql.\n: org.apache.spark.sql.AnalysisException: expression 'data.`excerpt`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;\nGlobalLimit 5\n+- LocalLimit 5\n   +- Aggregate [author#0], [author#0, excerpt#1, title#2, words#573, termfreq#579, tfidf#586, w2v#594, w2v_2D#603]\n      +- SubqueryAlias data\n         +- Project [author#0, excerpt#1, title#2, words#573, termfreq#579, tfidf#586, w2v#594, UDF(words#573) AS w2v_2D#603]\n            +- Project [author#0, excerpt#1, title#2, words#573, termfreq#579, tfidf#586, UDF(words#573) AS w2v#594]\n               +- Project [author#0, excerpt#1, title#2, words#573, termfreq#579, UDF(termfreq#579) AS tfidf#586]\n                  +- Project [author#0, excerpt#1, title#2, words#573, UDF(words#573) AS termfreq#579]\n                     +- Project [author#0, excerpt#1, title#2, f(excerpt#1) AS words#573]\n                        +- Relation[author#0,excerpt#1,title#2] json\n\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:57)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:245)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$11.apply(CheckAnalysis.scala:272)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$11.apply(CheckAnalysis.scala:272)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:272)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:127)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:127)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b2bb62a43585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mGROUP\u001b[0m \u001b[0mBY\u001b[0m \u001b[0mauthor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mLIMIT\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             ''').show()\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \"\"\"\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"expression 'data.`excerpt`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;\\nGlobalLimit 5\\n+- LocalLimit 5\\n   +- Aggregate [author#0], [author#0, excerpt#1, title#2, words#573, termfreq#579, tfidf#586, w2v#594, w2v_2D#603]\\n      +- SubqueryAlias data\\n         +- Project [author#0, excerpt#1, title#2, words#573, termfreq#579, tfidf#586, w2v#594, UDF(words#573) AS w2v_2D#603]\\n            +- Project [author#0, excerpt#1, title#2, words#573, termfreq#579, tfidf#586, UDF(words#573) AS w2v#594]\\n               +- Project [author#0, excerpt#1, title#2, words#573, termfreq#579, UDF(termfreq#579) AS tfidf#586]\\n                  +- Project [author#0, excerpt#1, title#2, words#573, UDF(words#573) AS termfreq#579]\\n                     +- Project [author#0, excerpt#1, title#2, f(excerpt#1) AS words#573]\\n                        +- Relation[author#0,excerpt#1,title#2] json\\n\""
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "spark.sql('''\n",
    "            SELECT *\n",
    "            FROM data\n",
    "            GROUP BY author\n",
    "            LIMIT 5\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
