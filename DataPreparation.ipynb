{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps    # import the spark suite\n",
    "import warnings         # display warning if spark context already exists\n",
    "import os\n",
    "\n",
    "# do I need to import string?\n",
    "import string\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "# not sure why Spark example uses HashingTF for TF-IDF \n",
    "# - scikit learn seems to recomend against this\n",
    "\n",
    "# from pyspark.ml.feature import HashingTF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ophidian/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: UserWarning: SparkContext already exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created SQLContext\n",
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- excerpt: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "row count:  9050\n",
      "+--------------+--------------------+---------------+\n",
      "|        author|             excerpt|          title|\n",
      "+--------------+--------------------+---------------+\n",
      "|CharlesDickens|A CHRISTMAS CAROL...|AChristmasCarol|\n",
      "|CharlesDickens|Mind! I don't mea...|AChristmasCarol|\n",
      "|CharlesDickens|Scrooge never pai...|AChristmasCarol|\n",
      "+--------------+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc = ps.SparkContext('local[4]') # create spark context locally on all available cpus\n",
    "    print('created SparkContext')\n",
    "except ValueError:\n",
    "    warnings.warn('SparkContext already exists')  # issue a warning if context already exists\n",
    "\n",
    "spark = ps.SQLContext(sc)\n",
    "print('created SQLContext')\n",
    "\n",
    "\n",
    "# Only Run This Once - It gets angry the second time!\n",
    "data_file = 'data/data.json'\n",
    "raw_df = spark.read.json(data_file)\n",
    "\n",
    "raw_df.printSchema()\n",
    "print \"row count: \", raw_df.count()\n",
    "raw_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create copy of raw_df incase I mess things up :P\n",
    "df = raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Process the Excerpts and Create New Columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add word count, sentence count, avg word len, avg sent len,  . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define functions to apply to a row\n",
    "\n",
    "def char_count(text):\n",
    "    return len(text)\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def avg_word_length(text):\n",
    "    return sum([len(t) for t in text.split()]) / float(len(text.split()))\n",
    "\n",
    "def sentence_count(text):\n",
    "    return len(text.split('.'))\n",
    "\n",
    "# ? Could use sentence count col and word count col to do thie w/o udf\n",
    "def sentence_length(text):\n",
    "    return sum([len(t.split()) for t in text.split('.')]) / float(len(text.split('.')))\n",
    "\n",
    "# ? Create count of paragraphs per excerpt?\n",
    "def paragraph_count(text):\n",
    "    pass\n",
    "\n",
    "# create User Defined Functions from above\n",
    "charcount_udf = udf(lambda x : char_count(x))\n",
    "wordcount_udf = udf(lambda x: word_count(x))\n",
    "avgwordlen_udf = udf(lambda x: avg_word_length(x))\n",
    "sentencecount_udf = udf(lambda x: sentence_count(x))\n",
    "sentencelength_udf = udf(lambda x: sentence_length(x))\n",
    "\n",
    "# add columns to datafram\n",
    "\n",
    "df = df.withColumn(\"char_count\", charcount_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"word_count\", wordcount_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"avg_wordlen\", avgwordlen_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"sent_count\", sentencecount_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"sent_length\", sentencelength_udf(df.excerpt).cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+----------+----------+----------+-----------+\n",
      "|        author|             excerpt|char_count|word_count|sent_count|sent_length|\n",
      "+--------------+--------------------+----------+----------+----------+-----------+\n",
      "|CharlesDickens|A CHRISTMAS CAROL...|    1156.0|     214.0|      13.0|   16.76923|\n",
      "|CharlesDickens|Mind! I don't mea...|    1504.0|     268.0|      14.0|  19.142857|\n",
      "|CharlesDickens|Scrooge never pai...|    1438.0|     250.0|      16.0|     15.625|\n",
      "|CharlesDickens|Nobody ever stopp...|    1643.0|     303.0|       9.0|  33.666668|\n",
      "|CharlesDickens|The door of Scroo...|    1141.0|     211.0|      10.0|       21.1|\n",
      "+--------------+--------------------+----------+----------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "spark.sql('''\n",
    "    SELECT author, excerpt, char_count, word_count, sent_count, sent_length\n",
    "    FROM data''').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For experimentation purposes - here is how to get one excerpt from the spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "df.createOrReplaceTempView(\"data\")\n",
    "temp = spark.sql('''SELECT excerpt FROM data LIMIT 5''')\n",
    "sample_text = str(temp.take(3)[1].excerpt)\n",
    "temp.show()\n",
    "sample_text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark's Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for feature in result.select(\"result\").take(3):\n",
    "    print(feature)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"label\", \"words\"])\n",
    "ngram = NGram(inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "for ngrams_label in ngramDataFrame.select(\"ngrams\", \"label\").take(3):\n",
    "    print(ngrams_label)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying NLP Functions to the Excerpts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will need to Remove Punctuation!!! (apparently spark tokenize doesn't do this for us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CONSIDER: reworking with REGEX\n",
    "# CONSIDER: Remove Punctuation with stopwords (Tip From Sally)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # return text.translate(None, punctuation)\n",
    "    return \"\".join(c for c in text if c not in set(string.punctuation))\n",
    "\n",
    "removepunctuation_udf = udf(lambda x : remove_punctuation(x))\n",
    "\n",
    "df = df.withColumn(\"words_only\", removepunctuation_udf(df.excerpt).cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(df.tokenized)\n",
    "\n",
    "# Tokenize (turn excerpt into list of words)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"words_only\", outputCol=\"tokenized\")\n",
    "df = tokenizer.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.createOrReplaceTempView(\"data\")\n",
    "# temp = spark.sql('''\n",
    "#     SELECT author, excerpt, words_only, tokenized\n",
    "#     FROM data\n",
    "#     LIMIT 6''')\n",
    "\n",
    "# temp.show()\n",
    "\n",
    "# temp.take(3)[1].words_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CountVectorize excerpts in df (turn wordlist into a vector of word counts)\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"tokenized\", outputCol=\"count_vectorized\")\n",
    "cvmodel = cv.fit(df)\n",
    "df = cvmodel.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate Inverse Document Frequency\n",
    "\n",
    "idf = IDF(inputCol=\"count_vectorized\", outputCol=\"tfidf\")\n",
    "idfmodel = idf.fit(df)\n",
    "df = idfmodel.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "sqldf = spark.sql('''\n",
    "    SELECT author, tokenized, count_vectorized, tfidf\n",
    "    FROM data\n",
    "    LIMIT 6\n",
    "    ''')\n",
    "sqldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
