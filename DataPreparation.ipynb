{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sql session setup by script:\t<pyspark.sql.session.SparkSession object at 0x109cdde90>\n",
      "spark context setup by script:\t<pyspark.context.SparkContext object at 0x109c3d9d0>\n",
      "pyspark imported by script:\t<module 'pyspark' from '/usr/local/Cellar/apache-spark/2 ...\n"
     ]
    }
   ],
   "source": [
    "# when starting jupyter with the sparkjupyter script, pyspark is already imported\n",
    "\n",
    "print \"sql session setup by script:\\t\", spark\n",
    "print \"spark context setup by script:\\t\", sc\n",
    "print \"pyspark imported by script:\\t\", str(pyspark)[:56], \"...\"\n",
    "\n",
    "# ps= pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- excerpt: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "row count:  9050\n",
      "+--------------+--------------------+---------------+\n",
      "|        author|             excerpt|          title|\n",
      "+--------------+--------------------+---------------+\n",
      "|CharlesDickens|A CHRISTMAS CAROL...|AChristmasCarol|\n",
      "|CharlesDickens|Mind! I don't mea...|AChristmasCarol|\n",
      "|CharlesDickens|Scrooge never pai...|AChristmasCarol|\n",
      "+--------------+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = 'data/data.json'\n",
    "raw_df = spark.read.json(data_file)\n",
    "\n",
    "raw_df.printSchema()\n",
    "print \"row count: \", raw_df.count()\n",
    "raw_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create copy of raw_df incase I mess things up :P\n",
    "df = raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Process the Excerpts and Create New Columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add word count, sentence count, avg word len, avg sent len,  . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define functions to apply to a row\n",
    "\n",
    "def char_count(text):\n",
    "    return len(text)\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def avg_word_length(text):\n",
    "    return sum([len(t) for t in text.split()]) / float(len(text.split()))\n",
    "\n",
    "def sentence_count(text):\n",
    "    return len(re.split('[?.!]', text))\n",
    "\n",
    "# ? Could use sentence count col and word count col to do thie w/o udf\n",
    "def sentence_length(text):\n",
    "    sentences = re.split('[?.!]', text)\n",
    "    return sum([len(t.split()) for t in sentences]) / float(len(sentences))\n",
    "\n",
    "# ? Create count of paragraphs per excerpt?\n",
    "def paragraph_count(text):\n",
    "    pass\n",
    "\n",
    "# create User Defined Functions from above\n",
    "charcount_udf = udf(lambda x : char_count(x))\n",
    "wordcount_udf = udf(lambda x: word_count(x))\n",
    "avgwordlen_udf = udf(lambda x: avg_word_length(x))\n",
    "sentencecount_udf = udf(lambda x: sentence_count(x))\n",
    "sentencelength_udf = udf(lambda x: sentence_length(x))\n",
    "\n",
    "# add columns to dataframe\n",
    "df = df.withColumn(\"char_count\", charcount_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"word_count\", wordcount_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"avg_wordlen\", avgwordlen_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"sent_count\", sentencecount_udf(df.excerpt).cast(FloatType())) \\\n",
    "        .withColumn(\"avg_sentlen\", sentencelength_udf(df.excerpt).cast(FloatType())).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FIXME: we are not actually calculating the averages properly here \n",
    "#  - this is an average of averages!\n",
    "\n",
    "df.createOrReplaceTempView(\"data\")\n",
    "spark.sql('''\n",
    "            SELECT author\n",
    "                    , ROUND(AVG(avg_wordlen),3) AS AvgWordLen\n",
    "                    , ROUND(AVG(word_count),1) AS AvgWordsPerPara\n",
    "                    , ROUND(AVG(sent_length),1) AS AvgWordsPerSent\n",
    "            FROM data\n",
    "            GROUP BY author\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For experimentation purposes - here is how to get one excerpt from the spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "df.createOrReplaceTempView(\"data\")\n",
    "temp = spark.sql('''SELECT excerpt FROM data LIMIT 5''')\n",
    "sample_text = str(temp.take(3)[1].excerpt)\n",
    "temp.show()\n",
    "sample_text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark's Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer  <br> regexTokenizer\n",
    "```python\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### word2vec\n",
    "\n",
    "```python\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for feature in result.select(\"result\").take(3):\n",
    "    print(feature)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### n-grams\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"label\", \"words\"])\n",
    "ngram = NGram(inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "for ngrams_label in ngramDataFrame.select(\"ngrams\", \"label\").take(3):\n",
    "    print(ngrams_label)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Sparks MLlib NLP Functions to the Excerpts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation from excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CONSIDER: reworking with REGEX\n",
    "# CONSIDER: Remove Punctuation with stopwords (Tip From Sally)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # return text.translate(None, punctuation)\n",
    "\n",
    "    # this avoids error in spark due to apparent 'translate' name collision\n",
    "    # Would it be more efficient to use .stip() on each word after tokenize?\n",
    "    return \"\".join(c for c in text if c not in set(string.punctuation))\n",
    "\n",
    "\n",
    "removepunctuation_udf = udf(lambda x : remove_punctuation(x))\n",
    "\n",
    "df = df.withColumn(\"words_only\", removepunctuation_udf(df.excerpt).cast(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the punctuationless excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(inputCol=\"words_only\", outputCol=\"tokenized\")\n",
    "df = tokenizer.transform(df).persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regexTokenizer\n",
    "#### May be worth playing with later - to split and remove punctuation in one step <br> and to handle cases like church-yard vs. instance--literally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df= df.drop('tokenized')\n",
    "\n",
    "# re_Tokenizer = RegexTokenizer(inputCol=\"excerpt\", outputCol=\"tokenized\", pattern=\"\\\\w+\")\n",
    "# # re_tokenizer = RegexTokenizer(inputCol=\"excerpt\", outputCol=\"tokenized\")\n",
    "# df = re_tokenizer.transform(df).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "temp = spark.sql('''\n",
    "    SELECT author, excerpt, tokenized\n",
    "    FROM data \n",
    "    LIMIT 10\n",
    "    ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words?\n",
    "##### Stop words left in for the moment on the theory/suspicion that authors' use these terms may be relevant to identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remover = StopWordsRemover(inputCol=\"tokenized\", outputCol=\"words_nostops\")\n",
    "# df = remover.transform(df).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorize token lists in df (turn wordlist into a vector of word counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cv = CountVectorizer(inputCol=\"tokenized\", outputCol=\"count_vectorized\")\n",
    "cvmodel = cv.fit(df)\n",
    "df = cvmodel.transform(df).persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "idf = IDF(inputCol=\"count_vectorized\", outputCol=\"tfidf\")\n",
    "idfmodel = idf.fit(df)\n",
    "df = idfmodel.transform(df).persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"tokenized\", outputCol=\"w2v\")\n",
    "w2vmodel = word2Vec.fit(df)\n",
    "df = w2vmodel.transform(df).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngram = NGram(n=2, inputCol=\"tokenized\", outputCol=\"2_gram\")\n",
    "df = ngram.transform(df)\n",
    "\n",
    "ngram = NGram(n=3, inputCol=\"tokenized\", outputCol=\"3_gram\")\n",
    "df = ngram.transform(df)\n",
    "\n",
    "ngram = NGram(n=4, inputCol=\"tokenized\", outputCol=\"4_gram\")\n",
    "df = ngram.transform(df)\n",
    "\n",
    "ngram = NGram(n=5, inputCol=\"tokenized\", outputCol=\"5_gram\")\n",
    "df = ngram.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The FINAL DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "sqldf = spark.sql('''\n",
    "    SELECT tokenized, count_vectorized, tfidf, w2v\n",
    "    FROM data\n",
    "    LIMIT 6\n",
    "    ''')\n",
    "sqldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|           tokenized|              2_gram|              5_gram|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[a, christmas, ca...|[a christmas, chr...|[a christmas caro...|\n",
      "|[mind, i, dont, m...|[mind i, i dont, ...|[mind i dont mean...|\n",
      "|[scrooge, never, ...|[scrooge never, n...|[scrooge never pa...|\n",
      "|[nobody, ever, st...|[nobody ever, eve...|[nobody ever stop...|\n",
      "|[the, door, of, s...|[the door, door o...|[the door of scro...|\n",
      "|[i, do, said, scr...|[i do, do said, s...|[i do said scroog...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "sqldf = spark.sql('''\n",
    "    SELECT tokenized, 2_gram, 5_gram\n",
    "    FROM data\n",
    "    LIMIT 6\n",
    "    ''')\n",
    "sqldf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqldf.write.save(\"data/save_test.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM parquet.`data/save_test.parquet`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqldf.createOrReplaceTempView(\"sqldf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# not too slow anymore\n",
    "spark.sql(\"SELECT tokenized FROM data LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VERY FAST (direct parquet access)\n",
    "spark.sql(\"SELECT tokenized FROM parquet.`data/save_test.parquet`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use formatting to set table name\n",
    "\n",
    "table = \"parquet.`data/save_test.parquet`\"\n",
    "\n",
    "spark.sql(\"SELECT * FROM {}\"\n",
    "          .format(table)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Data directly from Parquet - store data as pandas for plotting etc.\n",
    "\n",
    "(MOVE to Explore and Visualize notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pddf = spark.sql(\"SELECT * FROM {}\"\n",
    "          .format(table)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pddf.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Saving the Final DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').save(\"data/excerpt_df.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
