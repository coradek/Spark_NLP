{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "# import pyspark as ps    # import the spark suite\n",
    "# import warnings         # display warning if spark context already exists\n",
    "# import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "from pyspark.sql.functions import mean as sql_mean\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import colorsys\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that our database is saved as a parquet, we can query it directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+\n",
      "|        author|           tokenized|                 w2v|\n",
      "+--------------+--------------------+--------------------+\n",
      "|CharlesDickens|[a, christmas, ca...|[-0.1702252033501...|\n",
      "|CharlesDickens|[mind, i, dont, m...|[-0.2174013902028...|\n",
      "|CharlesDickens|[scrooge, never, ...|[-0.1526826002821...|\n",
      "|CharlesDickens|[nobody, ever, st...|[-0.1897002885570...|\n",
      "|CharlesDickens|[the, door, of, s...|[-0.1829826451034...|\n",
      "|CharlesDickens|[i, do, said, scr...|[-0.2629524400289...|\n",
      "|CharlesDickens|[uncle, pleaded, ...|[-0.2502119612455...|\n",
      "|CharlesDickens|[the, clerk, in, ...|[-0.2832240152638...|\n",
      "|CharlesDickens|[i, am, sorry, wi...|[-0.1989532466069...|\n",
      "|CharlesDickens|[mr, marley, has,...|[-0.2183633688141...|\n",
      "+--------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use formatting to set table name\n",
    "\n",
    "table = \"parquet.`data/excerpt_df.parquet`\"\n",
    "\n",
    "spark.sql('''\n",
    "    SELECT author, tokenized, w2v \n",
    "    FROM {}\n",
    "    LIMIT 10\n",
    "    '''.format(table)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# thankyou nick for the plotting code\n",
    "\n",
    "def make_plots(DFs, col, bins, title, colors, x_lab, authors, x_lim=None):\n",
    "    plt.subplots(4,4,sharex=True, sharey=True, figsize=(8,8))\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.94)\n",
    "    plt.suptitle(title, fontsize=18)\n",
    "    for i, DF in enumerate(DFs):\n",
    "        make_hist(DF, col, bins, colors[i], x_lim, i, x_lab, authors[i])\n",
    "    \n",
    "    \n",
    "def make_hist(DF, col, bins, color, x_lim, i, x_lab, author):\n",
    "    # Create pandas DF of bins and counts\n",
    "    temp_df = DF.select(col).toPandas()\n",
    "    \n",
    "    plt.subplot(2,2,i+1)\n",
    "    weights = temp_df.Frequency/temp_df.Frequency.sum()\n",
    "    plt.hist(temp_df[col], bins, normed=1, color=color, \n",
    "             alpha=0.2, label=author)\n",
    "    # Find mean sentence length\n",
    "    x_val = DF.select(sql_mean(col)).select('avg('+col+')').head()[0]\n",
    "    plt.axvline(x=x_val, color=color, label=\"Mean\")\n",
    "    plt.xlabel(x_lab)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlim(x_lim)\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello evan, are you a duck?\n",
      "+---------+--------------------+----------+-----------+----------+\n",
      "|   author|             excerpt|char_count|avg_wordlen|word_count|\n",
      "+---------+--------------------+----------+-----------+----------+\n",
      "|MarkTwain|A CONNECTICUT YAN...|    1275.0|   4.425532|     235.0|\n",
      "|MarkTwain|“You know about t...|    1529.0|   4.687732|     269.0|\n",
      "|MarkTwain|HOW SIR LAUNCELOT...|    3034.0|   4.188034|     585.0|\n",
      "|MarkTwain|And then they all...|    1574.0|  3.9684544|     317.0|\n",
      "|MarkTwain|As I laid the boo...|    1211.0|   4.454955|     222.0|\n",
      "|MarkTwain|Well, a man like ...|    1224.0|   4.212766|     235.0|\n",
      "|MarkTwain|“Fair sir, will y...|    1499.0|  3.8076923|     312.0|\n",
      "|MarkTwain|“Bridgeport?” sai...|    1197.0|   4.396396|     222.0|\n",
      "|MarkTwain| || THE TALE OF T...|    1999.0|   4.203125|     384.0|\n",
      "|MarkTwain|As we approached ...|    2457.0|   4.729604|     429.0|\n",
      "|MarkTwain| || CHAPTER II ||...|    1646.0|    4.14375|     320.0|\n",
      "|MarkTwain|“Go ‘long,” I sai...|    1089.0|   4.215311|     209.0|\n",
      "|MarkTwain|He said it wasn’t...|    1420.0|   4.262963|     270.0|\n",
      "|MarkTwain|Wherefore, being ...|    1166.0|   4.051948|     231.0|\n",
      "|MarkTwain|“My master and th...|    1201.0|   4.158798|     233.0|\n",
      "|MarkTwain|Get word to my fr...|    1422.0|  4.4942083|     259.0|\n",
      "|MarkTwain|In the middle of ...|    1535.0|  4.5451264|     277.0|\n",
      "|MarkTwain|As a rule, the sp...|    1512.0|  4.7748094|     262.0|\n",
      "|MarkTwain| || CHAPTER III |...|    1417.0|  4.7836733|     245.0|\n",
      "|MarkTwain|There was a fine ...|    1212.0|  4.6418605|     215.0|\n",
      "+---------+--------------------+----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'hello {}, are you a {}?'.format('evan', 'duck')\n",
    "\n",
    "author = \"'MarkTwain'\"\n",
    "\n",
    "spark.sql('''\n",
    "        SELECT author, excerpt, char_count, avg_wordlen, word_count\n",
    "        FROM {} \n",
    "        WHERE author = {} AND sent_length <=100\n",
    "        '''.format(table, author)).show()\n",
    "\n",
    "# spark.sql('''\n",
    "#         SELECT char_count, avg_wordlen, word_count, sent_length \n",
    "#         FROM parquet.`data/excerpt_df.parquet` \n",
    "#         WHERE author = 'MarkTwain' AND sent_length <=100\n",
    "#         ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"cannot resolve '`MarkTwain`' given input columns: [sent_count, tfidf, avg_wordlen, word_count, count_vectorized, words_only, excerpt, title, author, sent_length, char_count, tokenized, w2v]; line 4 pos 23\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f2282622b41f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtwaindf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MarkTwain'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmuirdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'JohnMuir'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0maustendf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'JaneAusten'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f2282622b41f>\u001b[0m in \u001b[0;36mmetadata_df\u001b[0;34m(author)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mSELECT\u001b[0m \u001b[0mcharacter_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_wordlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mFROM\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         WHERE author = {} AND sent_length <=100'''.format(table, author)).persist()\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.0.1/libexec/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \"\"\"\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.0.1/libexec/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.0.1/libexec/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"cannot resolve '`MarkTwain`' given input columns: [sent_count, tfidf, avg_wordlen, word_count, count_vectorized, words_only, excerpt, title, author, sent_length, char_count, tokenized, w2v]; line 4 pos 23\""
     ]
    }
   ],
   "source": [
    "def metadata_df(author):\n",
    "    return spark.sql('''\n",
    "        SELECT character_count, avg_wordlen, word_count, sent_length \n",
    "        FROM {} \n",
    "        WHERE author = {} AND sent_length <=100'''.format(table, author)).persist()\n",
    "\n",
    "\n",
    "\n",
    "twaindf = metadata_df('MarkTwain')\n",
    "muirdf = metadata_df('JohnMuir')\n",
    "austendf = metadata_df('JaneAusten')\n",
    "dickensdf = metadata_df('CharlesDickens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
