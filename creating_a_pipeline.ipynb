{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark and ML pipelines\n",
    "\n",
    "This is OPTIONAL reading for your information. Nothing in this notebook is required for the rest of the project to run.\n",
    "\n",
    "This notebook covers many of the basic functions of spark including:\n",
    "- Spark User defined functions: udf()\n",
    "- Spark Transformers\n",
    "- Customized Transformers\n",
    "- Spark Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType, IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.pipeline import Transformer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "from spacy.en import English\n",
    "\n",
    "from src.custom_transformers import SpacyTokenizer\n",
    "from src.nlp_pipeline import get_pipeline\n",
    "\n",
    "# from pyspark.ml.feature import Word2Vec\n",
    "# from pyspark.ml.feature import NGram\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sql session setup by script:\t <pyspark.sql.session.SparkSession object at 0x109fb9fd0>\n",
      "spark context setup by script:\t <pyspark.context.SparkContext object at 0x101adc278>\n",
      "pyspark imported by script:\t <module 'pyspark' from '/usr/local/Cellar/apache-spark/2 ...\n"
     ]
    }
   ],
   "source": [
    "# when starting jupyter with the sparkjupyter script, pyspark is already imported\n",
    "\n",
    "print(\"sql session setup by script:\\t\", spark)\n",
    "print(\"spark context setup by script:\\t\", sc)\n",
    "print(\"pyspark imported by script:\\t\", str(pyspark)[:56], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- excerpt: string (nullable = true)\n",
      " |-- excerpt_number: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "row count:  9050\n",
      "+--------------+--------------------+--------------+---------------+\n",
      "|        author|             excerpt|excerpt_number|          title|\n",
      "+--------------+--------------------+--------------+---------------+\n",
      "|CharlesDickens|A CHRISTMAS CAROL...|             0|AChristmasCarol|\n",
      "|CharlesDickens|Mind! I don't mea...|             1|AChristmasCarol|\n",
      "|CharlesDickens|Scrooge never pai...|             2|AChristmasCarol|\n",
      "+--------------+--------------------+--------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = 'data/excerpts.json'\n",
    "raw_df = spark.read.json(data_file)\n",
    "\n",
    "raw_df.printSchema()\n",
    "print(type(raw_df))\n",
    "print(\"row count: \", raw_df.count())\n",
    "raw_df.show(3)\n",
    "\n",
    "\n",
    "# create copy of raw_df incase I mess things up :P\n",
    "df = raw_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "20\n",
      "+--------------+--------------------+--------------------+--------------+\n",
      "|        author|               title|             excerpt|excerpt_number|\n",
      "+--------------+--------------------+--------------------+--------------+\n",
      "|CharlesDickens|     AChristmasCarol|It was not an agr...|            25|\n",
      "|CharlesDickens|    ATaleOfTwoCities|“So soon?” || Mis...|            25|\n",
      "|CharlesDickens|    DavidCopperfield|‘Peggotty,’ says ...|            25|\n",
      "|CharlesDickens|   GreatExpectations|“What’s in the bo...|            25|\n",
      "|CharlesDickens|         OliverTwist|'Walk in,' said t...|            25|\n",
      "|    JaneAusten|                Emma|She was so busy i...|            25|\n",
      "|    JaneAusten|       MansfieldPark|Fanny was too muc...|            25|\n",
      "|    JaneAusten|          Persuasion|But Mrs Clay was ...|            25|\n",
      "|    JaneAusten|   PrideAndPrejudice|“Not as you repre...|            25|\n",
      "|    JaneAusten| SenseAndSensibility|\"It is but a cott...|            25|\n",
      "|      JohnMuir|MyFirstSummerInTh...|How fine the weat...|            25|\n",
      "|      JohnMuir|            Stickeen|Thereafter Sticke...|            25|\n",
      "|      JohnMuir|TheStoryofMyBoyho...|Our amusements on...|            25|\n",
      "|      JohnMuir|         TheYosemite|The Royal Arch Fa...|            25|\n",
      "|      JohnMuir|     TravelsInAlaska|The very brightes...|            25|\n",
      "|     MarkTwain|AConnecticutYanke...|Now Sir Kay arose...|            25|\n",
      "|     MarkTwain|          RoughingIt|When a party camp...|            25|\n",
      "|     MarkTwain|TheAdventuresOfHu...|“How you talk, Hu...|            25|\n",
      "|     MarkTwain|  TheInnocentsAbroad|We steamed down t...|            25|\n",
      "|     MarkTwain|TheTragedyofPuddn...|\"Dey ain't but on...|            25|\n",
      "+--------------+--------------------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a tiny sample dataframe for testing\n",
    "\n",
    "\n",
    "# Random Sample:\n",
    "\n",
    "# tiny_df = df.sample(False, 1/1000).limit(5)\n",
    "# print(type(tiny_df))\n",
    "# print(tiny_df.count())\n",
    "# tiny_df.show()\n",
    "\n",
    "\n",
    "# One excerpt from each book\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "tiny_df = spark.sql(\"\"\"\n",
    "        SELECT author, title, excerpt, excerpt_number\n",
    "        FROM df\n",
    "        WHERE excerpt_number = 25\n",
    "        ORDER BY author, title\n",
    "        \"\"\").persist()\n",
    "\n",
    "\n",
    "print(type(tiny_df))\n",
    "print(tiny_df.count())\n",
    "tiny_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spacy: a brief aside\n",
    "\n",
    "Spacy is a production oriented Natural Language Processing package with (among other things) very nice tokenization options. I use spaCy here because it tokenizes punctuation and contractions better than spark's tokenizer.\n",
    "\n",
    "Here we will wrap the tokenization in a Spark UDF. Later we will include it in our customized transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 77.6 ms, sys: 20.9 ms, total: 98.4 ms\n",
      "Wall time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# timing to ensure spaCy is set up properly (should take ~100ms)\n",
    "\n",
    "parser = English()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Grab a couple excerpts for testing\n",
    "\n",
    "excerpt = df.take(100)[80]['excerpt']\n",
    "excerpt2 = df.take(100)[99]['excerpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'they', 'did', \"n't\", 'devote', 'the', 'whole', 'evening']\n",
      "CPU times: user 10.7 ms, sys: 2.21 ms, total: 12.9 ms\n",
      "Wall time: 13.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsedData = parser(excerpt)\n",
    "\n",
    "# sentences = [sent.string.strip() for sent in parsedData.sents]\n",
    "# for s in sentences:\n",
    "#     print(s, '\\n')\n",
    "\n",
    "tokens = [tok.lower_ for tok in parsedData]\n",
    "# print(type(token_lower[1]))\n",
    "print(tokens[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## UDF demonstration\n",
    "A quick way to create a User Defined Function (UDF) in spark:\n",
    "\n",
    "Get (or create a function) in python and use a lambda function to insert it in to \"udf(  )\".\n",
    "\n",
    "Don't forget to define your Spark DataType!\n",
    "\n",
    "```\n",
    "Other excerpt metadata to include via UDF:\n",
    "num_chars, num_words, num_sent, num_para\n",
    "(use these to calc word_len, word_per_sent, word_per_para, sent_per_para . . . etc.\n",
    "per excerpt, book and author)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------------------+--------------+--------------------+\n",
      "|        author|           title|             excerpt|excerpt_number|              tokens|\n",
      "+--------------+----------------+--------------------+--------------+--------------------+\n",
      "|CharlesDickens| AChristmasCarol|It was not an agr...|            25|[it, was, not, an...|\n",
      "|CharlesDickens|ATaleOfTwoCities|“So soon?” || Mis...|            25|[“, so, soon, ?, ...|\n",
      "|CharlesDickens|DavidCopperfield|‘Peggotty,’ says ...|            25|[‘, peggotty,’, s...|\n",
      "+--------------+----------------+--------------------+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "CPU times: user 21.3 ms, sys: 6.22 ms, total: 27.5 ms\n",
      "Wall time: 7.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def tokenize(text):\n",
    "    parser = English()\n",
    "    return [tok.lower_ for tok in parser(text)]\n",
    "\n",
    "tokenize_udf = udf(lambda x: tokenize(x), ArrayType(StringType()))\n",
    "\n",
    "df_tokens = tiny_df.withColumn(\"tokens\", tokenize_udf(df.excerpt))\n",
    "df_tokens.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Transformers in Spark\n",
    "A transformer is a function which takes a column from a dataframe, performs some action upon that column and attaches the result to the dataframe in a new column.\n",
    "\n",
    "## Native Transformers\n",
    "\n",
    "Many of the transformers in Spark's ML lib are great. Unfortunately Spark's tokenizer leaves punctuation attached to the adjacent word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------+--------------------+\n",
      "|        author|               title|             excerpt|excerpt_number|           tokenized|\n",
      "+--------------+--------------------+--------------------+--------------+--------------------+\n",
      "|CharlesDickens|     AChristmasCarol|It was not an agr...|            25|[it, was, not, an...|\n",
      "|CharlesDickens|    ATaleOfTwoCities|“So soon?” || Mis...|            25|[“so, soon?”, ||,...|\n",
      "|CharlesDickens|    DavidCopperfield|‘Peggotty,’ says ...|            25|[‘peggotty,’, say...|\n",
      "|CharlesDickens|   GreatExpectations|“What’s in the bo...|            25|[“what’s, in, the...|\n",
      "|CharlesDickens|         OliverTwist|'Walk in,' said t...|            25|['walk, in,', sai...|\n",
      "|    JaneAusten|                Emma|She was so busy i...|            25|[she, was, so, bu...|\n",
      "|    JaneAusten|       MansfieldPark|Fanny was too muc...|            25|[fanny, was, too,...|\n",
      "|    JaneAusten|          Persuasion|But Mrs Clay was ...|            25|[but, mrs, clay, ...|\n",
      "|    JaneAusten|   PrideAndPrejudice|“Not as you repre...|            25|[“not, as, you, r...|\n",
      "|    JaneAusten| SenseAndSensibility|\"It is but a cott...|            25|[\"it, is, but, a,...|\n",
      "|      JohnMuir|MyFirstSummerInTh...|How fine the weat...|            25|[how, fine, the, ...|\n",
      "|      JohnMuir|            Stickeen|Thereafter Sticke...|            25|[thereafter, stic...|\n",
      "|      JohnMuir|TheStoryofMyBoyho...|Our amusements on...|            25|[our, amusements,...|\n",
      "|      JohnMuir|         TheYosemite|The Royal Arch Fa...|            25|[the, royal, arch...|\n",
      "|      JohnMuir|     TravelsInAlaska|The very brightes...|            25|[the, very, brigh...|\n",
      "|     MarkTwain|AConnecticutYanke...|Now Sir Kay arose...|            25|[now, sir, kay, a...|\n",
      "|     MarkTwain|          RoughingIt|When a party camp...|            25|[when, a, party, ...|\n",
      "|     MarkTwain|TheAdventuresOfHu...|“How you talk, Hu...|            25|[“how, you, talk,...|\n",
      "|     MarkTwain|  TheInnocentsAbroad|We steamed down t...|            25|[we, steamed, dow...|\n",
      "|     MarkTwain|TheTragedyofPuddn...|\"Dey ain't but on...|            25|[\"dey, ain't, but...|\n",
      "+--------------+--------------------+--------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"excerpt\", outputCol=\"tokenized\")\n",
    "df_spark_tokens = tokenizer.transform(tiny_df)\n",
    "df_spark_tokens.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Customized Transformers\n",
    "Luckily, we can make our own transformers as well.\n",
    "\n",
    "Here we build the spaCy tokenizer (which treats punctuation as separate tokens) into a customized Spark transformer\n",
    "\n",
    "### Spacy Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 854 µs, sys: 599 µs, total: 1.45 ms\n",
      "Wall time: 1.68 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = SpacyTokenizer(inputCol='excerpt', outputCol='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+--------------------+--------------+--------------------+\n",
      "|        author|              title|             excerpt|excerpt_number|               words|\n",
      "+--------------+-------------------+--------------------+--------------+--------------------+\n",
      "|CharlesDickens|    AChristmasCarol|It was not an agr...|            25|[it, was, not, an...|\n",
      "|CharlesDickens|   ATaleOfTwoCities|“So soon?” || Mis...|            25|[“, so, soon, ?, ...|\n",
      "|CharlesDickens|   DavidCopperfield|‘Peggotty,’ says ...|            25|[‘, peggotty,’, s...|\n",
      "|CharlesDickens|  GreatExpectations|“What’s in the bo...|            25|[“, what, ’s, in,...|\n",
      "|CharlesDickens|        OliverTwist|'Walk in,' said t...|            25|[', walk, in, ,, ...|\n",
      "|    JaneAusten|               Emma|She was so busy i...|            25|[she, was, so, bu...|\n",
      "|    JaneAusten|      MansfieldPark|Fanny was too muc...|            25|[fanny, was, too,...|\n",
      "|    JaneAusten|         Persuasion|But Mrs Clay was ...|            25|[but, mrs, clay, ...|\n",
      "|    JaneAusten|  PrideAndPrejudice|“Not as you repre...|            25|[“, not, as, you,...|\n",
      "|    JaneAusten|SenseAndSensibility|\"It is but a cott...|            25|[\", it, is, but, ...|\n",
      "+--------------+-------------------+--------------------+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 23.5 ms, sys: 5.03 ms, total: 28.5 ms\n",
      "Wall time: 6.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_tokens = tokenizer.transform(tiny_df)\n",
    "df_tokens.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Pipelines in Spark\n",
    "\n",
    "Pipelines allow for multiple transformers to be strung together efficiently.\n",
    "\n",
    "By using \".getOutputCol( )\" column names can be set in a single location.\n",
    "\n",
    "Columns can then be added/dropped simply by adding or removing them from the \"stages\" list  in the Pipeline\n",
    "\n",
    "\n",
    "```python\n",
    "# Pipeline Example - \n",
    "# List all your transformers:\n",
    "tokenizer = RegexTokenizer(inputCol=\"parsed_text\", outputCol=\"raw_tokens\"\n",
    "            , pattern=\"\\\\W\", minTokenLength=3)\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='tokens_stop')\n",
    "stemmer = Stemming_Transformer(inputCol=remover.getOutputCol(), outputCol='tokens')\n",
    "bigram = NGram(inputCol=stemmer.getOutputCol(), outputCol='bigrams'\n",
    "         , n=2)\n",
    "trigram = NGram(inputCol=stemmer.getOutputCol(), outputCol='trigrams'\n",
    "          , n=3)\n",
    "cv = CountVectorizer(inputCol=stemmer.getOutputCol(), outputCol='token_countvector'\n",
    "     , minDF=10.0)\n",
    "idf = IDF(inputCol=cv.getOutputCol(), outputCol='token_idf'\n",
    "      , minDocFreq=10)\n",
    "w2v_2d = Word2Vec(vectorSize=2, minCount=2, inputCol=stemmer.getOutputCol()\n",
    "         , outputCol='word2vec_2d')\n",
    "w2v_large = Word2Vec(vectorSize=250, minCount=2, inputCol=stemmer.getOutputCol()\n",
    "            , outputCol='word2vec_large')\n",
    "\n",
    "# include desired transformers in the \"stages\" list\n",
    "pipe = Pipeline(stages=[tokenizer, remover, stemmer, cv, idf, w2v_2d, w2v_large])\n",
    "\n",
    "# and Voila! an entire dataframe can now be created with a single line of code.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here is a small functional pipeline example:\n",
    "\n",
    "# Set up transformers\n",
    "tokenizer = SpacyTokenizer(inputCol='excerpt', outputCol='words')\n",
    "countvec = CountVectorizer(inputCol=tokenizer.getOutputCol(), outputCol='termfreq')\n",
    "idf = IDF(inputCol=countvec.getOutputCol(), outputCol='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.4 ms, sys: 10.9 ms, total: 62.3 ms\n",
      "Wall time: 4.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now create the pipeline and build the dataframe by calling .fit() and .transform()\n",
    "pipeline = Pipeline(stages=[tokenizer, countvec, idf])\n",
    "sample_data = pipeline.fit(tiny_df).transform(tiny_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nlp_pipeline.py\n",
    "\n",
    "We can now put our entire pipeline in a script and apply it to new data with two lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- excerpt: string (nullable = true)\n",
      " |-- excerpt_number: long (nullable = true)\n",
      " |-- author_id: double (nullable = true)\n",
      " |-- title_id: double (nullable = true)\n",
      " |-- id_vector: vector (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- termfreq: vector (nullable = true)\n",
      " |-- tfidf: vector (nullable = true)\n",
      " |-- w2v: vector (nullable = true)\n",
      " |-- w2v_2d: vector (nullable = true)\n",
      "\n",
      "CPU times: user 103 ms, sys: 20.8 ms, total: 124 ms\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp_pipeline = get_pipeline()\n",
    "sample_data = nlp_pipeline.fit(tiny_df).transform(tiny_df)\n",
    "sample_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Section: SparkSQL\n",
    "\n",
    "\n",
    "Congratulation on reading this far! If you are familiar with SQL, here is a handy trick for viewing your data: Spark is fully SQL compatible! \n",
    "\n",
    "#### To view your dataframe as a SQL table run:\n",
    "\n",
    "```python\n",
    "df.createOrReplaceTempView(\"Table_Name\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM Table_Name\n",
    "        LIMIT 5\n",
    "        \"\"\").show()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "#### If your dataframe is saved as a parquet file it can be querried directly from disk:\n",
    "\n",
    "```python\n",
    "T = \"parquet.`path/to/dataframe.parquet`\"\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM {}\n",
    "        LIMIT 5\n",
    "        \"\"\".format(T)).show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+\n",
      "|        author|               title|               words|              w2v_2d|\n",
      "+--------------+--------------------+--------------------+--------------------+\n",
      "|CharlesDickens|     AChristmasCarol|[it, was, not, an...|[-0.0590827333764...|\n",
      "|CharlesDickens|    ATaleOfTwoCities|[“, so, soon, ?, ...|[-0.1139507902954...|\n",
      "|CharlesDickens|    DavidCopperfield|[‘, peggotty,’, s...|[-0.0822526921615...|\n",
      "|CharlesDickens|   GreatExpectations|[“, what, ’s, in,...|[-0.0584115711364...|\n",
      "|CharlesDickens|         OliverTwist|[', walk, in, ,, ...|[-0.0862761910674...|\n",
      "|    JaneAusten|                Emma|[she, was, so, bu...|[-0.1213585160292...|\n",
      "|    JaneAusten|       MansfieldPark|[fanny, was, too,...|[-0.0106421834240...|\n",
      "|    JaneAusten|          Persuasion|[but, mrs, clay, ...|[-0.0279451959840...|\n",
      "|    JaneAusten|   PrideAndPrejudice|[“, not, as, you,...|[-0.0371378898880...|\n",
      "|    JaneAusten| SenseAndSensibility|[\", it, is, but, ...|[-0.0628385479636...|\n",
      "|      JohnMuir|MyFirstSummerInTh...|[how, fine, the, ...|[-0.0962474407069...|\n",
      "|      JohnMuir|            Stickeen|[thereafter, stic...|[-0.0896949837592...|\n",
      "|      JohnMuir|TheStoryofMyBoyho...|[our, amusements,...|[-0.0853572436023...|\n",
      "|      JohnMuir|         TheYosemite|[the, royal, arch...|[-0.1433036355109...|\n",
      "|      JohnMuir|     TravelsInAlaska|[the, very, brigh...|[-0.1248807005115...|\n",
      "|     MarkTwain|AConnecticutYanke...|[now, sir, kay, a...|[-0.1062934178694...|\n",
      "|     MarkTwain|          RoughingIt|[when, a, party, ...|[-0.1081395022026...|\n",
      "|     MarkTwain|TheAdventuresOfHu...|[“, how, you, tal...|[-0.0739102522311...|\n",
      "|     MarkTwain|  TheInnocentsAbroad|[we, steamed, dow...|[-0.0858012253945...|\n",
      "|     MarkTwain|TheTragedyofPuddn...|[\", dey, ai, n't,...|[-0.0431099410476...|\n",
      "+--------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data.createOrReplaceTempView(\"nlp\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT author, title\n",
    "             , words, w2v_2d\n",
    "        FROM nlp\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
